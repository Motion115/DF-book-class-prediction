{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ctypes import CDLL\n",
    "import os\n",
    "\n",
    "## add dll for dgl\n",
    "tem = CDLL(r\"f:/anaconda3/envs/deep-learn/Lib/site-packages/dgl/dgl.dll\", winmode=0)\n",
    "os.add_dll_directory(r\"f:/anaconda3/envs/deep-learn/Lib/site-packages/dgl/dgl.dll\")\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as MF\n",
    "import tqdm\n",
    "from dgl.data import AsNodePredDataset\n",
    "from dgl.dataloading import (\n",
    "    DataLoader,\n",
    "    MultiLayerFullNeighborSampler,\n",
    "    NeighborSampler,\n",
    ")\n",
    "\n",
    "# import LabelPropagation\n",
    "from gtrick.dgl import LabelPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset suitable for DGL\n",
    "\n",
    "# edges = []\n",
    "# for index, row in data.iterrows():\n",
    "#     c = row[\"node_id\"]\n",
    "#     neighbour = eval(row[\"neighbour\"])\n",
    "#     for n in neighbour:\n",
    "#         edges.append([c,n])\n",
    "        \n",
    "# pd.DataFrame(edges).to_csv(\"data/edges.csv\",index=None,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dgl.graph\n",
    "data = pd.read_csv(\"data/Children.csv\",sep=\",\")\n",
    "data[\"label\"] = data[\"label\"].fillna(-1)\n",
    "\n",
    "edges = pd.read_csv(\"data/edges.csv\",header=None)\n",
    "g = dgl.graph((edges.iloc[:,0], edges.iloc[:,1]))\n",
    "g.ndata[\"label\"] = torch.from_numpy(data[\"label\"].to_numpy()).long()\n",
    "\n",
    "train_val_mask = g.ndata[\"label\"] != -1\n",
    "test_mask = g.ndata[\"label\"] == -1\n",
    "train_val_idx = torch.nonzero(train_val_mask).squeeze()\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1.0 - train_ratio\n",
    "train_size = int(train_ratio * train_val_idx.shape[0])\n",
    "val_size = train_val_idx.shape[0] - train_size\n",
    "\n",
    "random_indices = torch.randperm(train_val_idx.shape[0])\n",
    "train_idx = train_val_idx[random_indices[:train_size]]\n",
    "val_idx = train_val_idx[random_indices[train_size:]]\n",
    "\n",
    "train_mask = torch.zeros_like(g.ndata[\"label\"])\n",
    "train_mask[train_idx] = True\n",
    "\n",
    "val_mask = torch.zeros_like(g.ndata[\"label\"])\n",
    "val_mask[val_idx] = True\n",
    "\n",
    "g.ndata[\"train_mask\"] = train_mask\n",
    "g.ndata[\"val_mask\"] = val_mask\n",
    "g.ndata[\"test_mask\"] = test_mask\n",
    "g.ndata[\"feat\"] = torch.tensor(torch.load(\"data/bert-cls-embeddings.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "train_idx = torch.nonzero(g.ndata[\"train_mask\"]).squeeze()\n",
    "val_idx = torch.nonzero(g.ndata[\"val_mask\"]).squeeze()\n",
    "test_idx = torch.nonzero(g.ndata[\"test_mask\"]).squeeze()\n",
    "\n",
    "train_X, val_X, train_y, val_y = g.ndata[\"feat\"][train_idx], g.ndata[\"feat\"][val_idx], g.ndata[\"label\"][train_idx], g.ndata[\"label\"][val_idx], "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 1.31387\n",
      "[2]\tvalid_0's multi_logloss: 1.18682\n",
      "[3]\tvalid_0's multi_logloss: 1.07347\n",
      "[4]\tvalid_0's multi_logloss: 1.00416\n",
      "[5]\tvalid_0's multi_logloss: 0.948575\n",
      "[6]\tvalid_0's multi_logloss: 0.904282\n",
      "[7]\tvalid_0's multi_logloss: 0.90139\n",
      "[8]\tvalid_0's multi_logloss: 0.925054\n",
      "[9]\tvalid_0's multi_logloss: 0.893115\n",
      "[10]\tvalid_0's multi_logloss: 0.910852\n",
      "[11]\tvalid_0's multi_logloss: 0.871206\n",
      "[12]\tvalid_0's multi_logloss: 0.891732\n",
      "[13]\tvalid_0's multi_logloss: 0.91558\n",
      "[14]\tvalid_0's multi_logloss: 0.947161\n",
      "[15]\tvalid_0's multi_logloss: 0.95021\n",
      "[16]\tvalid_0's multi_logloss: 1.0515\n",
      "[17]\tvalid_0's multi_logloss: 1.13138\n",
      "[18]\tvalid_0's multi_logloss: 1.27464\n",
      "[19]\tvalid_0's multi_logloss: 1.20725\n",
      "[20]\tvalid_0's multi_logloss: 1.54218\n",
      "[21]\tvalid_0's multi_logloss: 1.69283\n",
      "[22]\tvalid_0's multi_logloss: 1.54685\n",
      "[23]\tvalid_0's multi_logloss: 1.5841\n",
      "[24]\tvalid_0's multi_logloss: 2.12095\n",
      "[25]\tvalid_0's multi_logloss: 1.96487\n",
      "[26]\tvalid_0's multi_logloss: 2.40823\n",
      "[27]\tvalid_0's multi_logloss: 2.55898\n",
      "[28]\tvalid_0's multi_logloss: 2.97997\n",
      "[29]\tvalid_0's multi_logloss: 3.47142\n",
      "[30]\tvalid_0's multi_logloss: 2.92477\n",
      "[31]\tvalid_0's multi_logloss: 4.04934\n",
      "[32]\tvalid_0's multi_logloss: 3.14999\n",
      "[33]\tvalid_0's multi_logloss: 3.25667\n",
      "[34]\tvalid_0's multi_logloss: 4.11117\n",
      "[35]\tvalid_0's multi_logloss: 3.48451\n",
      "[36]\tvalid_0's multi_logloss: 3.51473\n",
      "[37]\tvalid_0's multi_logloss: 3.67824\n",
      "[38]\tvalid_0's multi_logloss: 4.12539\n",
      "[39]\tvalid_0's multi_logloss: 4.37368\n",
      "[40]\tvalid_0's multi_logloss: 4.49777\n",
      "[41]\tvalid_0's multi_logloss: 4.48157\n",
      "[42]\tvalid_0's multi_logloss: 4.54753\n",
      "[43]\tvalid_0's multi_logloss: 4.33307\n",
      "[44]\tvalid_0's multi_logloss: 4.81386\n",
      "[45]\tvalid_0's multi_logloss: 4.6032\n",
      "[46]\tvalid_0's multi_logloss: 4.6876\n",
      "[47]\tvalid_0's multi_logloss: 4.67128\n",
      "[48]\tvalid_0's multi_logloss: 4.96378\n",
      "[49]\tvalid_0's multi_logloss: 4.91131\n",
      "[50]\tvalid_0's multi_logloss: 5.05748\n",
      "[51]\tvalid_0's multi_logloss: 5.58812\n",
      "[52]\tvalid_0's multi_logloss: 5.35897\n",
      "[53]\tvalid_0's multi_logloss: 5.24894\n",
      "[54]\tvalid_0's multi_logloss: 5.06404\n",
      "[55]\tvalid_0's multi_logloss: 4.99499\n",
      "[56]\tvalid_0's multi_logloss: 4.76717\n",
      "[57]\tvalid_0's multi_logloss: 5.13351\n",
      "[58]\tvalid_0's multi_logloss: 4.78478\n",
      "[59]\tvalid_0's multi_logloss: 5.15568\n",
      "[60]\tvalid_0's multi_logloss: 4.93537\n",
      "[61]\tvalid_0's multi_logloss: 5.16991\n",
      "[62]\tvalid_0's multi_logloss: 6.75522\n",
      "[63]\tvalid_0's multi_logloss: 7.56131\n",
      "[64]\tvalid_0's multi_logloss: 7.33286\n",
      "[65]\tvalid_0's multi_logloss: 6.84233\n",
      "[66]\tvalid_0's multi_logloss: 6.90103\n",
      "[67]\tvalid_0's multi_logloss: 6.84681\n",
      "[68]\tvalid_0's multi_logloss: 6.50174\n",
      "[69]\tvalid_0's multi_logloss: 6.19536\n",
      "[70]\tvalid_0's multi_logloss: 5.91789\n",
      "[71]\tvalid_0's multi_logloss: 6.02759\n",
      "[72]\tvalid_0's multi_logloss: 5.67223\n",
      "[73]\tvalid_0's multi_logloss: 5.7148\n",
      "[74]\tvalid_0's multi_logloss: 5.85867\n",
      "[75]\tvalid_0's multi_logloss: 6.59612\n",
      "[76]\tvalid_0's multi_logloss: 6.36235\n",
      "[77]\tvalid_0's multi_logloss: 10.5944\n",
      "[78]\tvalid_0's multi_logloss: 15.9645\n",
      "[79]\tvalid_0's multi_logloss: 21.1635\n",
      "[80]\tvalid_0's multi_logloss: 14.9063\n",
      "[81]\tvalid_0's multi_logloss: 11.5296\n",
      "[82]\tvalid_0's multi_logloss: 10.6551\n",
      "[83]\tvalid_0's multi_logloss: 9.87547\n",
      "[84]\tvalid_0's multi_logloss: 11.0207\n",
      "[85]\tvalid_0's multi_logloss: 8.92296\n",
      "[86]\tvalid_0's multi_logloss: 8.36233\n",
      "[87]\tvalid_0's multi_logloss: 7.61848\n",
      "[88]\tvalid_0's multi_logloss: 7.57326\n",
      "[89]\tvalid_0's multi_logloss: 7.5589\n",
      "[90]\tvalid_0's multi_logloss: 7.5589\n",
      "[91]\tvalid_0's multi_logloss: 7.5589\n",
      "[92]\tvalid_0's multi_logloss: 7.5589\n",
      "[93]\tvalid_0's multi_logloss: 7.5589\n",
      "[94]\tvalid_0's multi_logloss: 7.5589\n",
      "[95]\tvalid_0's multi_logloss: 7.5589\n",
      "[96]\tvalid_0's multi_logloss: 7.5589\n",
      "[97]\tvalid_0's multi_logloss: 7.5589\n",
      "[98]\tvalid_0's multi_logloss: 7.5589\n",
      "[99]\tvalid_0's multi_logloss: 7.5589\n",
      "[100]\tvalid_0's multi_logloss: 7.5589\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {'n_estimators': 300,'learning_rate': 0.1,'class_weight': \"balanced\",\n",
    "            'reg_alpha': 0.25, 'reg_lambda': 0.2,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.7,\n",
    "            'max_depth': 4, 'num_leaves': 4,\n",
    "            'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc'}\n",
    "\n",
    "lgb_model = LGBMClassifier()\n",
    "lgb_model.fit(train_X, train_y, eval_set=[(val_X, val_y)])\n",
    "pred_y = lgb_model.predict_proba(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7811382113821138"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(torch.argmax(torch.tensor(pred_y),dim=1), g.ndata[\"label\"][val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1,  ..., 2, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_size, hid_size, \"mean\"))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, hid_size, \"mean\"))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, out_size, \"mean\"))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.hid_size = hid_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h = layer(block, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = F.relu(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, device, batch_size):\n",
    "        feat = g.ndata[\"feat\"]\n",
    "        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=[\"feat\"])\n",
    "        dataloader = DataLoader(\n",
    "            g,\n",
    "            torch.arange(g.num_nodes()).to(g.device),\n",
    "            sampler,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        buffer_device = torch.device(\"cpu\")\n",
    "        pin_memory = buffer_device != device\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = torch.empty(\n",
    "                g.num_nodes(),\n",
    "                self.hid_size if l != len(self.layers) - 1 else self.out_size,\n",
    "                dtype=feat.dtype,\n",
    "                device=buffer_device,\n",
    "                pin_memory=pin_memory,\n",
    "            )\n",
    "            feat = feat.to(device)\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                x = feat[input_nodes]\n",
    "                h = layer(blocks[0], x)\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = F.relu(h)\n",
    "                    h = self.dropout(h)\n",
    "                y[output_nodes[0] : output_nodes[-1] + 1] = h.to(buffer_device)\n",
    "            feat = y\n",
    "        return y\n",
    "\n",
    "\n",
    "def evaluate(model, graph, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    y_hats = []\n",
    "    for it, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            x = blocks[0].srcdata[\"feat\"]\n",
    "            ys.append(blocks[-1].dstdata[\"label\"])\n",
    "            y_hats.append(model(blocks, x))\n",
    "\n",
    "    # label propagation\n",
    "    # lp_layers, lp_alpah = 50, 0.9\n",
    "    # lp = LabelPropagation(lp_layers, lp_alpah)\n",
    "    # yh = lp(graph, graph.ndata[\"label\"], mask=graph.ndata[\"train_mask\"])\n",
    "\n",
    "    return MF.accuracy(\n",
    "        torch.cat(y_hats),\n",
    "        torch.cat(ys),\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "\n",
    "def layerwise_infer(device, graph, model, num_classes, batch_size):\n",
    "    test_idx = torch.nonzero(g.ndata[\"test_mask\"]).squeeze().to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.inference(graph, device, batch_size)\n",
    "        return pred\n",
    "        \n",
    "def train(device, g, model, num_classes):\n",
    "    train_idx = torch.nonzero(g.ndata[\"train_mask\"]).squeeze().to(device)\n",
    "    val_idx = torch.nonzero(g.ndata[\"val_mask\"]).squeeze().to(device)\n",
    "    sampler = NeighborSampler(\n",
    "        [20, 20, 20], \n",
    "        prefetch_node_feats=[\"feat\"],\n",
    "        prefetch_labels=[\"label\"],\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        g,\n",
    "        train_idx,\n",
    "        sampler,\n",
    "        device=device,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        g,\n",
    "        val_idx,\n",
    "        sampler,\n",
    "        device=device,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for it, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):\n",
    "            x = blocks[0].srcdata[\"feat\"]\n",
    "            y = blocks[-1].dstdata[\"label\"]\n",
    "            y_hat = model(blocks, x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        acc = evaluate(model, g, val_dataloader, num_classes)\n",
    "        print(\n",
    "            \"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} \".format(\n",
    "                epoch, total_loss / (it + 1), acc.item()\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g.to(\"cpu\")\n",
    "num_classes = 24\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "in_size = g.ndata[\"feat\"].shape[1]\n",
    "out_size = num_classes\n",
    "model = SAGE(in_size, 800, out_size).to(device)\n",
    "\n",
    "print(\"Training...\")\n",
    "train(device, g, model, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:03<00:00,  6.31it/s]\n",
      "100%|██████████| 19/19 [00:01<00:00, 12.16it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 19.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "print(\"Testing...\")\n",
    "pred = layerwise_infer(\n",
    "    device, g, model, num_classes, batch_size=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = torch.argmax(pred,dim=1)\n",
    "test_idx = torch.nonzero(g.ndata[\"test_mask\"]).squeeze()\n",
    "\n",
    "result = pd.DataFrame({\n",
    "    \"node_id\":test_idx,\n",
    "    \"label\":pred_label[test_idx]\n",
    "})\n",
    "\n",
    "result.to_csv(\"result/submission.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
